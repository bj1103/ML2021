# -*- coding: utf-8 -*-
"""hw7_bert

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW07/HW07.ipynb

# **Homework 7 - Bert (Question Answering)**

If you have any questions, feel free to email us at ntu-ml-2021spring-ta@googlegroups.com



Slide:    [Link](https://docs.google.com/presentation/d/1aQoWogAQo_xVJvMQMrGaYiWzuyfO0QyLLAhiMwFyS2w)　Kaggle: [Link](https://www.kaggle.com/c/ml2021-spring-hw7)　Data: [Link](https://drive.google.com/uc?id=1znKmX08v9Fygp-dgwo7BKiLIf2qL1FH1)

## Task description
- Chinese Extractive Question Answering
  - Input: Paragraph + Question
  - Output: Answer

- Objective: Learn how to fine tune a pretrained model on downstream task using transformers

- Todo
    - Fine tune a pretrained chinese BERT model
    - Change hyperparameters (e.g. doc_stride)
    - Apply linear learning rate decay
    - Try other pretrained models
    - Improve preprocessing
    - Improve postprocessing
- Training tips
    - Automatic mixed precision
    - Gradient accumulation
    - Ensemble

- Estimated training time (tesla t4 with automatic mixed precision enabled)
    - Simple: 8mins
    - Medium: 8mins
    - Strong: 25mins
    - Boss: 2hrs

## Download Dataset
"""

# Download link 1
# !gdown --id '1znKmX08v9Fygp-dgwo7BKiLIf2qL1FH1' --output hw7_data.zip

# Download Link 2 (if the above link fails) 
# !gdown --id '1pOu3FdPdvzielUZyggeD7KDnVy9iW1uC' --output hw7_data.zip

# !unzip -o hw7_data.zip

# For this HW, K80 < P4 < T4 < P100 <= T4(fp16) < V100
# !nvidia-smi

"""## Install transformers

Documentation for the toolkit:　https://huggingface.co/transformers/
"""

# You are allowed to change version of transformers or use other toolkits
# !pip install transformers==4.5.0

"""## Import Packages"""

import json
import numpy as np
import random
import torch
import math
from torch.utils.data import DataLoader, Dataset 
from transformers import AdamW, BertForQuestionAnswering, BertTokenizerFast, AutoModelForQuestionAnswering
import os
from tqdm.auto import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"

# Fix random seed for reproducibility
def same_seeds(seed):
	  torch.manual_seed(seed)
	  if torch.cuda.is_available():
		    torch.cuda.manual_seed(seed)
		    torch.cuda.manual_seed_all(seed)
	  np.random.seed(seed)
	  random.seed(seed)
	  torch.backends.cudnn.benchmark = False
	  torch.backends.cudnn.deterministic = True
same_seeds(655371)

# Change "fp16_training" to True to support automatic mixed precision training (fp16)	
fp16_training = True

if fp16_training:
    # !pip install accelerate==0.2.0
    from accelerate import Accelerator
    accelerator = Accelerator(fp16=True)
    device = accelerator.device

# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/

"""## Load Model and Tokenizer




 
"""

model = AutoModelForQuestionAnswering.from_pretrained("hfl/chinese-macbert-large").to(device)
tokenizer = BertTokenizerFast.from_pretrained("hfl/chinese-macbert-large")

# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)

"""## Read Data

- Training set: 26935 QA pairs
- Dev set: 3523  QA pairs
- Test set: 3492  QA pairs

- {train/dev/test}_questions:	
  - List of dicts with the following keys:
   - id (int)
   - paragraph_id (int)
   - question_text (string)
   - answer_text (string)
   - answer_start (int)
   - answer_end (int)
- {train/dev/test}_paragraphs: 
  - List of strings
  - paragraph_ids in questions correspond to indexs in paragraphs
  - A paragraph may be used by several questions 
"""

def read_data(file):
    with open(file, 'r', encoding="utf-8") as reader:
        data = json.load(reader)
    return data["questions"], data["paragraphs"]

prepath = './'
train_questions, train_paragraphs = read_data(prepath+"hw7_train.json")
dev_questions, dev_paragraphs = read_data(prepath+"hw7_dev.json")
test_questions, test_paragraphs = read_data(prepath+"hw7_test.json")

"""## Tokenize Data"""

# Tokenize questions and paragraphs separately
# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__ 

train_questions_tokenized = tokenizer([train_question["question_text"] for train_question in train_questions], add_special_tokens=False)
dev_questions_tokenized = tokenizer([dev_question["question_text"] for dev_question in dev_questions], add_special_tokens=False)
test_questions_tokenized = tokenizer([test_question["question_text"] for test_question in test_questions], add_special_tokens=False) 

train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)
dev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False)
test_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False)

# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model

"""## Dataset and Dataloader"""

class QA_Dataset(Dataset):
    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):
        self.split = split
        self.questions = questions
        self.tokenized_questions = tokenized_questions
        self.tokenized_paragraphs = tokenized_paragraphs
        self.max_question_len = 50
        self.max_paragraph_len = 450
        
        ##### TODO: Change value of doc_stride #####
        self.doc_stride = 100

        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]
        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1

    def __len__(self):
        return len(self.questions)

    def __getitem__(self, idx):
        question = self.questions[idx]
        tokenized_question = self.tokenized_questions[idx]
        tokenized_paragraph = self.tokenized_paragraphs[question["paragraph_id"]]

        ##### TODO: Preprocessing #####
        # Hint: How to prevent model from learning something it should not learn
        if self.split == "train":
            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  
            answer_start_token = tokenized_paragraph.char_to_token(question["answer_start"])
            answer_end_token = tokenized_paragraph.char_to_token(question["answer_end"])

            # A single window is obtained by slicing the portion of paragraph containing the answer
            mid = (answer_start_token + answer_end_token) // 2
            # paragraph_start = max(0, min(mid - self.max_paragraph_len // 2, len(tokenized_paragraph) - self.max_paragraph_len))
            paragraph_start = 0
            paragraph_end = paragraph_start + self.max_paragraph_len
            
            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)
            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] 
            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]		
            
            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  
            answer_start_token += len(input_ids_question) - paragraph_start
            answer_end_token += len(input_ids_question) - paragraph_start

            if answer_start_token < len(input_ids_question) or answer_end_token >= len(input_ids_question) + len(input_ids_paragraph)-1:
                answer_start_token = tokenized_paragraph.char_to_token(question["answer_start"])
                answer_end_token = tokenized_paragraph.char_to_token(question["answer_end"])
                paragraph_start = len(tokenized_paragraph) - self.max_paragraph_len
                paragraph_end = paragraph_start + self.max_paragraph_len
                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)
                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] 
                input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]		
                # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  
                answer_start_token += len(input_ids_question) - paragraph_start
                answer_end_token += len(input_ids_question) - paragraph_start
                if answer_start_token < len(input_ids_question) or answer_end_token >= len(input_ids_question) + len(input_ids_paragraph)-1:
                    answer_start_token = tokenized_paragraph.char_to_token(question["answer_start"])
                    answer_end_token = tokenized_paragraph.char_to_token(question["answer_end"])
                    paragraph_start = mid - self.max_paragraph_len // 2
                    paragraph_end = paragraph_start + self.max_paragraph_len
                    # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)
                    input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] 
                    input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]		
                    # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  
                    answer_start_token += len(input_ids_question) - paragraph_start
                    answer_end_token += len(input_ids_question) - paragraph_start
                    if answer_start_token < len(input_ids_question) or answer_end_token >= len(input_ids_question) + len(input_ids_paragraph)-1:
                        print('it should not happen!')
                        print(len(tokenized_paragraph.ids))
                        print(len(tokenized_question.ids))
                        print(tokenized_paragraph.char_to_token(question["answer_start"]))
                        print(tokenized_paragraph.char_to_token(question["answer_end"]))
                        # print(tokenized_paragraph[answer_start_token:answer_end_token])
                        exit()

                        answer_start_token = 0
                        answer_end_token = 0
            
            # Pad sequence and obtain inputs to model 
            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)
            assert(len(input_ids) <= 512)
            

            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token

        # Validation/Testing
        else:
            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []
            start_ids = []
            q_len = []
            # Paragraph is split into several windows, each with start positions separated by step "doc_stride"
            # for i in range(0, len(tokenized_paragraph), self.doc_stride):
                
            #     # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)
            #     input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]
            #     input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]
                
            #     # Pad sequence and obtain inputs to model
            #     input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)
                
            #     input_ids_list.append(input_ids)
            #     token_type_ids_list.append(token_type_ids)
            #     attention_mask_list.append(attention_mask)
            
            paragraph_start = 0
            paragraph_end = paragraph_start + self.max_paragraph_len
            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] 
            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]		
            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)
            input_ids_list.append(input_ids)
            token_type_ids_list.append(token_type_ids)
            attention_mask_list.append(attention_mask)
            start_ids.append(paragraph_start)
            q_len.append(len(input_ids_question))

            # if (len(tokenized_paragraph) - self.max_paragraph_len > 0):
            paragraph_start = max(0, len(tokenized_paragraph) - self.max_paragraph_len)
            paragraph_end = paragraph_start + self.max_paragraph_len
            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] 
            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]		
            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)
            input_ids_list.append(input_ids)
            token_type_ids_list.append(token_type_ids)
            attention_mask_list.append(attention_mask)
            start_ids.append(paragraph_start)
            q_len.append(len(input_ids_question))
            
            if (len(tokenized_paragraph) - 2 * self.max_paragraph_len > 0):
                paragraph_start = self.max_paragraph_len // 2
                paragraph_end = paragraph_start + self.max_paragraph_len
                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] 
                input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]		
                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)
                input_ids_list.append(input_ids)
                token_type_ids_list.append(token_type_ids)
                attention_mask_list.append(attention_mask)
                start_ids.append(paragraph_start)
                q_len.append(len(input_ids_question))

            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list), start_ids, q_len

    def padding(self, input_ids_question, input_ids_paragraph):
        # Pad zeros if sequence length is shorter than max_seq_len
        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)
        # Indices of input sequence tokens in the vocabulary
        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len
        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]
        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len
        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]
        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len
        
        return input_ids, token_type_ids, attention_mask

train_set = QA_Dataset("train", train_questions, train_questions_tokenized, train_paragraphs_tokenized)
dev_set = QA_Dataset("dev", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)
test_set = QA_Dataset("test", test_questions, test_questions_tokenized, test_paragraphs_tokenized)

train_batch_size = 4

# Note: Do NOT change batch size of dev_loader / test_loader !
# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair
train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)
dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)
test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)

"""## Function for Evaluation"""

def evaluate(data, output):
    ##### TODO: Postprocessing #####
    # There is a bug and room for improvement in postprocessing 
    # Hint: Open your prediction file to see what is wrong 
    
    answer = ''
    max_prob = float('-inf')
    num_of_windows = data[0].shape[1]
    ans_start_prob = 0
    ans_end_prob = 0
    ans_data = ''
    
    for k in range(num_of_windows):
        # Obtain answer by choosing the most probable start position / end position
        # start_prob, start_index = torch.max(output.start_logits[k], dim=0)
        # end_prob, end_index = torch.max(output.end_logits[k], dim=0)
        # output.start_logits[k] = torch.nn.functional.softmax(output.start_logits[k], dim=0)
        # output.end_logits[k] = torch.nn.functional.softmax(output.end_logits[k], dim=0)

        start, indice_start = torch.sort(output.start_logits[k], dim=0, descending = True)
        end, indice_end = torch.sort(output.end_logits[k], dim=0, descending = True)
        start_prob = start[0]
        end_prob = end[0]
        start_index = indice_start[0]
        end_index = indice_end[0]

        if indice_end[0] - indice_start[0] > 30 or indice_end[0] - indice_start[0] <= 0:
          score_1 = start[1] + end[0]
          score_2 = start[0] + end[1]
          score_3 = start[1] + end[1]
          if score_1 > score_2 and score_1 > score_3 and 0 < indice_end[0] - indice_start[1] < 30:
              start_prob = start[1]
              end_prob = end[0]
              start_index = indice_start[1]
              end_index = indice_end[0]
          elif score_2 > score_1 and score_2 > score_3 and 0 < indice_end[1] - indice_start[0] < 30:
              start_prob = start[0]
              end_prob = end[1]
              start_index = indice_start[0]
              end_index = indice_end[1]
          elif  0 < indice_end[1] - indice_start[1] < 30:
              start_prob = start[1]
              end_prob = end[1]
              start_index = indice_start[1]
              end_index = indice_end[1]
        
        # Probability of answer is calculated as sum of start_prob and end_prob
        prob = start_prob + end_prob
        if end_index <= start_index or end_index == 0 or start_index == 0:
            prob = 0
        
        
        # Replace answer if calculated probability is larger than previous windows
        if prob > max_prob:
            max_prob = prob
            # Convert tokens to chars (e.g. [1920, 7032] --> "大 金")
            answer = tokenizer.decode(data[0][0][k][start_index : end_index + 1]).replace(' ','')
            ans_data = tokenizer.decode(data[0][0][k]).replace(' ','')
            ans_start_prob = start_index.item() + data[3][k] - data[4][k]
            ans_end_prob = end_index.item() + data[3][k] - data[4][k]
    # if answer != target:
    #     print('data : ', ans_data)
    #     print('target : ', target)
    #     print('predict : ', answer)
    #     print('start prob : ', ans_start_prob.item())
    #     print('end prob : ', ans_end_prob.item())
    #     # print('k :', k)
    #     print()
    # Remove spaces in answer (e.g. "大 金" --> "大金")
    # ans = answer.replace(' ','')
    # print('predict:', ans)
    
    return answer, ans_start_prob, ans_end_prob


from torch.optim import Optimizer
from torch.optim.lr_scheduler import LambdaLR

def get_cosine_schedule_with_warmup(
    optimizer: Optimizer,
    num_warmup_steps: int,
    num_training_steps: int,
    num_cycles: float = 0.5,
    last_epoch: int = -1,
):
  

    def lr_lambda(current_step):
        # Warmup
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        # decadence
        progress = float(current_step - num_warmup_steps) / float(
            max(1, num_training_steps - num_warmup_steps)
        )
        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))

    return LambdaLR(optimizer, lr_lambda, last_epoch)


"""## Training"""

num_epoch = 20
validation = True
logging_step = 100
learning_rate = 1e-5
optimizer = AdamW(model.parameters(), lr=learning_rate)
warmup_step = 2000
accum_step = 32
config = {
    'num_epoch' : num_epoch,
    'learning_rate' : learning_rate,
    'warmup_step' : warmup_step,
    'accum_step' : accum_step
}
print(config)

total_step = len(train_loader) * num_epoch / accum_step
scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_step, total_step)

if fp16_training:
    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader) 

model.train()

print("Start Training ...")
step = 1
for epoch in range(num_epoch):
    train_loss = train_acc = 0
    
    for data in tqdm(train_loader):	
        # Load all data into GPU
        data = [i.to(device) for i in data]
        
        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only "input_ids" is mandatory)
        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)  
        output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])

        # Choose the most probable start position / end position
        start_index = torch.argmax(output.start_logits, dim=1)
        end_index = torch.argmax(output.end_logits, dim=1)
        
        # Prediction is correct only if both start_index and end_index are correct
        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()
        
        train_loss += output.loss
        output.loss = output.loss / accum_step
        
        if fp16_training:
            accelerator.backward(output.loss)
        else:
            output.loss.backward()
        
        if (step % accum_step == 0) or (step + 1 == len(train_loader)):
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
        step += 1
        
        ##### TODO: Apply linear learning rate decay #####
        # optimizer.param_groups[0]["lr"] -= learning_rate / total_step
        
        # Print training loss and accuracy over past logging step
        if step % logging_step == 0:
            print(f"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}")
            train_loss = train_acc = 0
    
    print("Saving Model ...")
    model_save_dir = "./drive/MyDrive/ML/HW7/saved_model_" + str(epoch)
    os.mkdir(model_save_dir)
    model.save_pretrained(model_save_dir)
    
    if validation:
        print("Evaluating Dev Set ...")
        model.eval()
        with torch.no_grad():
            dev_acc = 0
            for i, data in enumerate(tqdm(dev_loader)):
                output = model(input_ids=data[0].squeeze().to(device), token_type_ids=data[1].squeeze().to(device),
                       attention_mask=data[2].squeeze().to(device))
                # prediction is correct only if answer text exactly matches
                dev_acc += evaluate(data, output) == dev_questions[i]["answer_text"]
            print(f"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}")
        model.train()

# Save a model and its configuration file to the directory 「saved_model」 
# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」
# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained("saved_model")」
    

"""## Testing"""

print("Evaluating Test Set ...")

result = []

model.eval()
with torch.no_grad():
    dev_acc = 0
    for i, data in enumerate(tqdm(dev_loader)):
        output = model(input_ids=data[0].squeeze().to(device), token_type_ids=data[1].squeeze().to(device),
                attention_mask=data[2].squeeze().to(device))
        # prediction is correct only if answer text exactly matches
        ans, start_index, end_index = evaluate(data, output) 
       
        s = dev_paragraphs_tokenized[dev_questions[i]["paragraph_id"]].token_to_chars(start_index)[0]
        e = dev_paragraphs_tokenized[dev_questions[i]["paragraph_id"]].token_to_chars(end_index)[1]
        ans_n = dev_paragraphs[dev_questions[i]["paragraph_id"]][s:e]
        if ans != dev_questions[i]["answer_text"]:
          print('new: ', ans_n)
          print('old: ', ans)
          print('target: ', dev_questions[i]["answer_text"])
        
        dev_acc += ans_n == dev_questions[i]["answer_text"]
        
        # print('correct ans:', dev_questions[i]["answer_text"])
        
    print(f"Validation | acc = {dev_acc / len(dev_loader):.3f}")

with torch.no_grad():
    for data in tqdm(test_loader):
        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),
                       attention_mask=data[2].squeeze(dim=0).to(device))
        result.append(evaluate(data, output))

result_file = "result.csv"
with open(result_file, 'w') as f:	
	  f.write("ID,Answer\n")
	  for i, test_question in enumerate(test_questions):
        # Replace commas in answers with empty strings (since csv is separated by comma)
        # Answers in kaggle are processed in the same way
		    f.write(f"{test_question['id']},{result[i].replace(',','')}\n")